#   temph <- lapply(hNodes, FUN = function(x) {
#       heading <- lapply(x, FUN = function(y) {xpathSApply(y, "//th", xmlValue)})
#       print(heading)
#   })
#   tempv <- lapply(nodes, FUN = function(x) {
#       value <- lapply(x, FUN = function(y) {xpathSApply(y, "//td", xmlValue)})
#   })
#print(length(nodes))
#heading <- lapply(nodes, FUN = function(x) {xpathSApply(x, "//th", xmlValue)})[i]
#heading <- apply(nodes, 2, FUN = function(x) {xpathSApply(x, "//th", xmlValue)})
#value <- as.list(xpathSApply(nodes[[i]], "//td", xmlValue))[[i]]
#value <- lapply(nodes, FUN = function(x) {xpathSApply(x, "//td", xmlValue)})[i]
#if (value == "n/a") value = "NA"
#heading <- gsub(":", "", heading, fixed=TRUE)
#value <- gsub("n/a", "NA", value)    # change NA string to something more R-friendly
#value <- gsub(")(", ");(", value, fixed=TRUE)    # separate coordinates with a semicolon
#print(c(class(value), value))
#temp <- data.frame(heading = heading, value = value, stringsAsFactors=FALSE)
## build temporary data frame and merge with existing data frame
#if (is.null(data)) data <- temp
#else data <- rbind(data, temp)
#}
#data
}
## Scrapes project, name, type, dataURL link, metadata URL link from a given NRCAN URL
## PRE:     url - the URL to the website featuring the list of projects (upto 50 projects)
## POST:    data frame containing the projects, names, types, dataLinks, and metaLinks displayed on the given URL page
scrapeNRCANProjectData <- function(url) {
library(XML)
library(utils)
## parse the HTML
html <- htmlTreeParse(url, useInternalNodes=TRUE)
## extract nodes for each data set on given webpage
nodes <- getNodeSet(html, "//table[@id='results-table']/tbody/tr")
data <- NULL
## loop through data set nodes
for (i in 1:length(nodes)) {
## extract project, name, data type, and metadata link
project <- xpathSApply(nodes[[i]], "//td", xmlValue)[((i-1)*5)+1]
name <- xpathSApply(nodes[[i]], "//td", xmlValue)[((i-1)*5)+2]
type <- xpathSApply(nodes[[i]], "//td", xmlValue) [((i-1)*5)+3]
dataLink <- paste("http://gdr.agg.nrcan.gc.ca/gdrdap/dap/search-eng.php?tree-0=Click+here+for+more+options&tree-1=Click+here+for+more+options&tree-2=Click+here+for+more+options&tree-3=Click+here+for+more+options&datatype-ddl=&layer_name=", URLencode(name, reserved = TRUE), "&submit_search=Submit+Search#results", sep="")
metaLink <- xpathSApply(nodes[[i]], "//td/a[@href]", xmlGetAttr, "href")[i]
metaLink <- paste("http://gdr.agg.nrcan.gc.ca/gdrdap/dap/", metaLink, sep="")
## build temporary data frame, and merge with existing data frame
temp <- data.frame(project, name, type, dataLink, metaLink, stringsAsFactors = FALSE)
if (is.null(data)) data <- temp
else data <- rbind(data, temp)
}
data
}
NRCANGeophysicsScrape()
?as.data.frame
?lapply
?frameapply
?ldply
library(plyr)
?ldply
## TO DO:
##      scrapeNRCANMetadata() seems to work on its own
##      Test it as being called from NRCANGeophysicsScrape()
##      Add result to the main data frame
##      Think about parsing out the survey polygon or how to do it
## Performs webscraping on NRCAN's geophysics data repository site, retrieving a list of data projects and their
## metadata
## PRE:     (optional) url - the url to the NRCAN geophysics data repository site
## POST:    data frame containing all data set projects available through NRCAN and their metadata
NRCANGeophysicsScrape <- function(url = "http://gdr.agg.nrcan.gc.ca/gdrdap/dap/search-eng.php") {
library(XML)
library(dplyr)
## scrape initial page
data <- scrapeNRCANProjectData(url)
## parse the HTML and build nodes
html <- htmlTreeParse(url, useInternalNodes=TRUE)
nodes <- getNodeSet(html, "//table[@id='results-table']/tbody/tr")
counter <- 1
## loop through project results pages
##      NRCAN will only show 50 results per page
##      At time of coding, 8217 results total
##      So loop through page showing results 1-50, 51-100, etc.
##      Loop will timeout after 250 iterations if continues finding results
while(length(nodes) > 0 & counter < 2) {
## parse the html and build nodes
nextUrl <- paste(url, "?page=", counter, sep="")
html <- htmlTreeParse(nextUrl, useInternalNodes=TRUE)
nodes <- getNodeSet(html, "//table[@id='results-table']/tbody/tr")
## if there are projects listed, merge them with our data frame
if (length(nodes) > 0) data <- rbind(data, scrapeNRCANProjectData(nextUrl))
print(c("master set", dim(data)))
counter <- counter + 1
}
data <- parseMetadata(data)
data
}
parseMetadata <- function(data) {
numCol <- ncol(data)
#print(length(data$metaLink))
#metadata <- apply(data$metaLink, 1, FUN = scrapeNRCANMetadata)
metadata <- scrapeNRCANMetadata(data$metaLink)
#print(head(metadata,25))
#for (i in 1:nrow(data)) {
#    metadata <- scrapeNRCANMetadata(data$metaLink[i])
#    print(class(metadata))
#    print(nrow(metadata))
#    print(head(metadata,3))
#    for (j in 1:nrow(metadata)) {
#        data[i, numCol+j] <- metadata$value[j]
#    }
#    if (i == 1) {
#        names(data) <- c(names(data)[1:numCol], metadata$heading)
#    }
#}
#data
}
## Scrapes the NRCAN metadata from the given metadata URL link
scrapeNRCANMetadata <- function(url) {
library(XML)
library(plyr)
library(dplyr)
## parse the HTML
#html <- htmlTreeParse(url, useInternalNodes=TRUE)
value <- NULL
i <- 0
html <- lapply(url, FUN = function(x) {
htmlTreeParse(x, useInternalNodes=TRUE)
})
#hNodes <- getNodeSet(html, "//table[1]/tr/th")
hNodes <- lapply(html, FUN = function(x) {getNodeSet(x, "//table[1]/tr/th")})
#nodes <- getNodeSet(html, "//table[1]/tr")
nodes <- lapply(html, FUN = function(x) {getNodeSet(x, "//table[1]/tr")})
data <- NULL
metadata <- ldply(nodes, FUN = function(x) {
temp2 <- NULL
for (i in 1:length(x)) {
heading <- xpathSApply(x[[i]], "//th", xmlValue)[i]
value <- xpathSApply(x[[i]], "//td", xmlValue)[i]
temp <- data.frame(value, stringsAsFactors=FALSE)
names(temp) <- heading
if (is.null(temp2)) temp2 <- temp
else temp2 <- as.data.frame(cbind(temp2, temp))
}
if (is.null(data)) data <- temp2
else data <- as.data.frame(rbind(data, temp2))
data
})
#metadata <- unlist(metadata)
print(c("class metadata before case: ", class(metadata)))
#metadata <- as.data.frame(metadata)
print(c("length metadata after caset ", length(metadata)))
print(c("dim metadata after cast: ", dim(metadata)))
print(c("dim metadata element 1: ", dim(metadata[[1]])))
#print(head(metadata, 5))
#for(i in 1:length(nodes)) {
## extract heading/value pairs from the metadata
#heading <- xpathSApply(nodes[[i]], "//th", xmlValue)[i]
#   temph <- lapply(hNodes, FUN = function(x) {
#       heading <- lapply(x, FUN = function(y) {xpathSApply(y, "//th", xmlValue)})
#       print(heading)
#   })
#   tempv <- lapply(nodes, FUN = function(x) {
#       value <- lapply(x, FUN = function(y) {xpathSApply(y, "//td", xmlValue)})
#   })
#print(length(nodes))
#heading <- lapply(nodes, FUN = function(x) {xpathSApply(x, "//th", xmlValue)})[i]
#heading <- apply(nodes, 2, FUN = function(x) {xpathSApply(x, "//th", xmlValue)})
#value <- as.list(xpathSApply(nodes[[i]], "//td", xmlValue))[[i]]
#value <- lapply(nodes, FUN = function(x) {xpathSApply(x, "//td", xmlValue)})[i]
#if (value == "n/a") value = "NA"
#heading <- gsub(":", "", heading, fixed=TRUE)
#value <- gsub("n/a", "NA", value)    # change NA string to something more R-friendly
#value <- gsub(")(", ");(", value, fixed=TRUE)    # separate coordinates with a semicolon
#print(c(class(value), value))
#temp <- data.frame(heading = heading, value = value, stringsAsFactors=FALSE)
## build temporary data frame and merge with existing data frame
#if (is.null(data)) data <- temp
#else data <- rbind(data, temp)
#}
#data
}
## Scrapes project, name, type, dataURL link, metadata URL link from a given NRCAN URL
## PRE:     url - the URL to the website featuring the list of projects (upto 50 projects)
## POST:    data frame containing the projects, names, types, dataLinks, and metaLinks displayed on the given URL page
scrapeNRCANProjectData <- function(url) {
library(XML)
library(utils)
## parse the HTML
html <- htmlTreeParse(url, useInternalNodes=TRUE)
## extract nodes for each data set on given webpage
nodes <- getNodeSet(html, "//table[@id='results-table']/tbody/tr")
data <- NULL
## loop through data set nodes
for (i in 1:length(nodes)) {
## extract project, name, data type, and metadata link
project <- xpathSApply(nodes[[i]], "//td", xmlValue)[((i-1)*5)+1]
name <- xpathSApply(nodes[[i]], "//td", xmlValue)[((i-1)*5)+2]
type <- xpathSApply(nodes[[i]], "//td", xmlValue) [((i-1)*5)+3]
dataLink <- paste("http://gdr.agg.nrcan.gc.ca/gdrdap/dap/search-eng.php?tree-0=Click+here+for+more+options&tree-1=Click+here+for+more+options&tree-2=Click+here+for+more+options&tree-3=Click+here+for+more+options&datatype-ddl=&layer_name=", URLencode(name, reserved = TRUE), "&submit_search=Submit+Search#results", sep="")
metaLink <- xpathSApply(nodes[[i]], "//td/a[@href]", xmlGetAttr, "href")[i]
metaLink <- paste("http://gdr.agg.nrcan.gc.ca/gdrdap/dap/", metaLink, sep="")
## build temporary data frame, and merge with existing data frame
temp <- data.frame(project, name, type, dataLink, metaLink, stringsAsFactors = FALSE)
if (is.null(data)) data <- temp
else data <- rbind(data, temp)
}
data
}
NRCANGeophysicsScrape()
## TO DO:
##      scrapeNRCANMetadata() seems to work on its own
##      Test it as being called from NRCANGeophysicsScrape()
##      Add result to the main data frame
##      Think about parsing out the survey polygon or how to do it
## Performs webscraping on NRCAN's geophysics data repository site, retrieving a list of data projects and their
## metadata
## PRE:     (optional) url - the url to the NRCAN geophysics data repository site
## POST:    data frame containing all data set projects available through NRCAN and their metadata
NRCANGeophysicsScrape <- function(url = "http://gdr.agg.nrcan.gc.ca/gdrdap/dap/search-eng.php") {
library(XML)
library(dplyr)
## scrape initial page
data <- scrapeNRCANProjectData(url)
## parse the HTML and build nodes
html <- htmlTreeParse(url, useInternalNodes=TRUE)
nodes <- getNodeSet(html, "//table[@id='results-table']/tbody/tr")
counter <- 1
## loop through project results pages
##      NRCAN will only show 50 results per page
##      At time of coding, 8217 results total
##      So loop through page showing results 1-50, 51-100, etc.
##      Loop will timeout after 250 iterations if continues finding results
while(length(nodes) > 0 & counter < 2) {
## parse the html and build nodes
nextUrl <- paste(url, "?page=", counter, sep="")
html <- htmlTreeParse(nextUrl, useInternalNodes=TRUE)
nodes <- getNodeSet(html, "//table[@id='results-table']/tbody/tr")
## if there are projects listed, merge them with our data frame
if (length(nodes) > 0) data <- rbind(data, scrapeNRCANProjectData(nextUrl))
print(c("master set", dim(data)))
counter <- counter + 1
}
data <- parseMetadata(data)
data
}
parseMetadata <- function(data) {
numCol <- ncol(data)
#print(length(data$metaLink))
#metadata <- apply(data$metaLink, 1, FUN = scrapeNRCANMetadata)
metadata <- scrapeNRCANMetadata(data$metaLink)
#print(head(metadata,25))
#for (i in 1:nrow(data)) {
#    metadata <- scrapeNRCANMetadata(data$metaLink[i])
#    print(class(metadata))
#    print(nrow(metadata))
#    print(head(metadata,3))
#    for (j in 1:nrow(metadata)) {
#        data[i, numCol+j] <- metadata$value[j]
#    }
#    if (i == 1) {
#        names(data) <- c(names(data)[1:numCol], metadata$heading)
#    }
#}
#data
}
## Scrapes the NRCAN metadata from the given metadata URL link
scrapeNRCANMetadata <- function(url) {
library(XML)
library(plyr)
library(dplyr)
## parse the HTML
#html <- htmlTreeParse(url, useInternalNodes=TRUE)
value <- NULL
i <- 0
html <- lapply(url, FUN = function(x) {
htmlTreeParse(x, useInternalNodes=TRUE)
})
#hNodes <- getNodeSet(html, "//table[1]/tr/th")
hNodes <- lapply(html, FUN = function(x) {getNodeSet(x, "//table[1]/tr/th")})
#nodes <- getNodeSet(html, "//table[1]/tr")
nodes <- lapply(html, FUN = function(x) {getNodeSet(x, "//table[1]/tr")})
data <- NULL
metadata <- ldply(nodes, .fun = function(x) {
temp2 <- NULL
for (i in 1:length(x)) {
heading <- xpathSApply(x[[i]], "//th", xmlValue)[i]
value <- xpathSApply(x[[i]], "//td", xmlValue)[i]
temp <- data.frame(value, stringsAsFactors=FALSE)
names(temp) <- heading
if (is.null(temp2)) temp2 <- temp
else temp2 <- as.data.frame(cbind(temp2, temp))
}
if (is.null(data)) data <- temp2
else data <- as.data.frame(rbind(data, temp2))
data
})
#metadata <- unlist(metadata)
print(c("class metadata before case: ", class(metadata)))
#metadata <- as.data.frame(metadata)
print(c("length metadata after caset ", length(metadata)))
print(c("dim metadata after cast: ", dim(metadata)))
print(c("dim metadata element 1: ", dim(metadata[[1]])))
#print(head(metadata, 5))
#for(i in 1:length(nodes)) {
## extract heading/value pairs from the metadata
#heading <- xpathSApply(nodes[[i]], "//th", xmlValue)[i]
#   temph <- lapply(hNodes, FUN = function(x) {
#       heading <- lapply(x, FUN = function(y) {xpathSApply(y, "//th", xmlValue)})
#       print(heading)
#   })
#   tempv <- lapply(nodes, FUN = function(x) {
#       value <- lapply(x, FUN = function(y) {xpathSApply(y, "//td", xmlValue)})
#   })
#print(length(nodes))
#heading <- lapply(nodes, FUN = function(x) {xpathSApply(x, "//th", xmlValue)})[i]
#heading <- apply(nodes, 2, FUN = function(x) {xpathSApply(x, "//th", xmlValue)})
#value <- as.list(xpathSApply(nodes[[i]], "//td", xmlValue))[[i]]
#value <- lapply(nodes, FUN = function(x) {xpathSApply(x, "//td", xmlValue)})[i]
#if (value == "n/a") value = "NA"
#heading <- gsub(":", "", heading, fixed=TRUE)
#value <- gsub("n/a", "NA", value)    # change NA string to something more R-friendly
#value <- gsub(")(", ");(", value, fixed=TRUE)    # separate coordinates with a semicolon
#print(c(class(value), value))
#temp <- data.frame(heading = heading, value = value, stringsAsFactors=FALSE)
## build temporary data frame and merge with existing data frame
#if (is.null(data)) data <- temp
#else data <- rbind(data, temp)
#}
#data
}
## Scrapes project, name, type, dataURL link, metadata URL link from a given NRCAN URL
## PRE:     url - the URL to the website featuring the list of projects (upto 50 projects)
## POST:    data frame containing the projects, names, types, dataLinks, and metaLinks displayed on the given URL page
scrapeNRCANProjectData <- function(url) {
library(XML)
library(utils)
## parse the HTML
html <- htmlTreeParse(url, useInternalNodes=TRUE)
## extract nodes for each data set on given webpage
nodes <- getNodeSet(html, "//table[@id='results-table']/tbody/tr")
data <- NULL
## loop through data set nodes
for (i in 1:length(nodes)) {
## extract project, name, data type, and metadata link
project <- xpathSApply(nodes[[i]], "//td", xmlValue)[((i-1)*5)+1]
name <- xpathSApply(nodes[[i]], "//td", xmlValue)[((i-1)*5)+2]
type <- xpathSApply(nodes[[i]], "//td", xmlValue) [((i-1)*5)+3]
dataLink <- paste("http://gdr.agg.nrcan.gc.ca/gdrdap/dap/search-eng.php?tree-0=Click+here+for+more+options&tree-1=Click+here+for+more+options&tree-2=Click+here+for+more+options&tree-3=Click+here+for+more+options&datatype-ddl=&layer_name=", URLencode(name, reserved = TRUE), "&submit_search=Submit+Search#results", sep="")
metaLink <- xpathSApply(nodes[[i]], "//td/a[@href]", xmlGetAttr, "href")[i]
metaLink <- paste("http://gdr.agg.nrcan.gc.ca/gdrdap/dap/", metaLink, sep="")
## build temporary data frame, and merge with existing data frame
temp <- data.frame(project, name, type, dataLink, metaLink, stringsAsFactors = FALSE)
if (is.null(data)) data <- temp
else data <- rbind(data, temp)
}
data
}
NRCANGeophysicsScrape()
## TO DO:
##      scrapeNRCANMetadata() seems to work on its own
##      Test it as being called from NRCANGeophysicsScrape()
##      Add result to the main data frame
##      Think about parsing out the survey polygon or how to do it
## Performs webscraping on NRCAN's geophysics data repository site, retrieving a list of data projects and their
## metadata
## PRE:     (optional) url - the url to the NRCAN geophysics data repository site
## POST:    data frame containing all data set projects available through NRCAN and their metadata
NRCANGeophysicsScrape <- function(url = "http://gdr.agg.nrcan.gc.ca/gdrdap/dap/search-eng.php") {
library(XML)
library(dplyr)
## scrape initial page
data <- scrapeNRCANProjectData(url)
## parse the HTML and build nodes
html <- htmlTreeParse(url, useInternalNodes=TRUE)
nodes <- getNodeSet(html, "//table[@id='results-table']/tbody/tr")
counter <- 1
## loop through project results pages
##      NRCAN will only show 50 results per page
##      At time of coding, 8217 results total
##      So loop through page showing results 1-50, 51-100, etc.
##      Loop will timeout after 250 iterations if continues finding results
while(length(nodes) > 0 & counter < 2) {
## parse the html and build nodes
nextUrl <- paste(url, "?page=", counter, sep="")
html <- htmlTreeParse(nextUrl, useInternalNodes=TRUE)
nodes <- getNodeSet(html, "//table[@id='results-table']/tbody/tr")
## if there are projects listed, merge them with our data frame
if (length(nodes) > 0) data <- rbind(data, scrapeNRCANProjectData(nextUrl))
print(c("master set", dim(data)))
counter <- counter + 1
}
data <- parseMetadata(data)
print(head(data,2))
}
parseMetadata <- function(data) {
numCol <- ncol(data)
#print(length(data$metaLink))
#metadata <- apply(data$metaLink, 1, FUN = scrapeNRCANMetadata)
metadata <- scrapeNRCANMetadata(data$metaLink)
data <- cbind(data, metadata)
data
#print(head(metadata,25))
#for (i in 1:nrow(data)) {
#    metadata <- scrapeNRCANMetadata(data$metaLink[i])
#    print(class(metadata))
#    print(nrow(metadata))
#    print(head(metadata,3))
#    for (j in 1:nrow(metadata)) {
#        data[i, numCol+j] <- metadata$value[j]
#    }
#    if (i == 1) {
#        names(data) <- c(names(data)[1:numCol], metadata$heading)
#    }
#}
#data
}
## Scrapes the NRCAN metadata from the given metadata URL link
scrapeNRCANMetadata <- function(url) {
library(XML)
library(plyr)
library(dplyr)
## parse the HTML
#html <- htmlTreeParse(url, useInternalNodes=TRUE)
value <- NULL
i <- 0
html <- lapply(url, FUN = function(x) {
htmlTreeParse(x, useInternalNodes=TRUE)
})
#hNodes <- getNodeSet(html, "//table[1]/tr/th")
hNodes <- lapply(html, FUN = function(x) {getNodeSet(x, "//table[1]/tr/th")})
#nodes <- getNodeSet(html, "//table[1]/tr")
nodes <- lapply(html, FUN = function(x) {getNodeSet(x, "//table[1]/tr")})
data <- NULL
metadata <- ldply(nodes, .fun = function(x) {
temp2 <- NULL
for (i in 1:length(x)) {
heading <- xpathSApply(x[[i]], "//th", xmlValue)[i]
value <- xpathSApply(x[[i]], "//td", xmlValue)[i]
temp <- data.frame(value, stringsAsFactors=FALSE)
names(temp) <- heading
if (is.null(temp2)) temp2 <- temp
else temp2 <- as.data.frame(cbind(temp2, temp))
}
if (is.null(data)) data <- temp2
else data <- as.data.frame(rbind(data, temp2))
data
})
metadata
#metadata <- unlist(metadata)
#print(c("class metadata before case: ", class(metadata)))
#metadata <- as.data.frame(metadata)
#print(c("length metadata after caset ", length(metadata)))
#print(c("dim metadata after cast: ", dim(metadata)))
#print(c("dim metadata element 1: ", dim(metadata[[1]])))
#print(head(metadata, 5))
#for(i in 1:length(nodes)) {
## extract heading/value pairs from the metadata
#heading <- xpathSApply(nodes[[i]], "//th", xmlValue)[i]
#   temph <- lapply(hNodes, FUN = function(x) {
#       heading <- lapply(x, FUN = function(y) {xpathSApply(y, "//th", xmlValue)})
#       print(heading)
#   })
#   tempv <- lapply(nodes, FUN = function(x) {
#       value <- lapply(x, FUN = function(y) {xpathSApply(y, "//td", xmlValue)})
#   })
#print(length(nodes))
#heading <- lapply(nodes, FUN = function(x) {xpathSApply(x, "//th", xmlValue)})[i]
#heading <- apply(nodes, 2, FUN = function(x) {xpathSApply(x, "//th", xmlValue)})
#value <- as.list(xpathSApply(nodes[[i]], "//td", xmlValue))[[i]]
#value <- lapply(nodes, FUN = function(x) {xpathSApply(x, "//td", xmlValue)})[i]
#if (value == "n/a") value = "NA"
#heading <- gsub(":", "", heading, fixed=TRUE)
#value <- gsub("n/a", "NA", value)    # change NA string to something more R-friendly
#value <- gsub(")(", ");(", value, fixed=TRUE)    # separate coordinates with a semicolon
#print(c(class(value), value))
#temp <- data.frame(heading = heading, value = value, stringsAsFactors=FALSE)
## build temporary data frame and merge with existing data frame
#if (is.null(data)) data <- temp
#else data <- rbind(data, temp)
#}
#data
}
## Scrapes project, name, type, dataURL link, metadata URL link from a given NRCAN URL
## PRE:     url - the URL to the website featuring the list of projects (upto 50 projects)
## POST:    data frame containing the projects, names, types, dataLinks, and metaLinks displayed on the given URL page
scrapeNRCANProjectData <- function(url) {
library(XML)
library(utils)
## parse the HTML
html <- htmlTreeParse(url, useInternalNodes=TRUE)
## extract nodes for each data set on given webpage
nodes <- getNodeSet(html, "//table[@id='results-table']/tbody/tr")
data <- NULL
## loop through data set nodes
for (i in 1:length(nodes)) {
## extract project, name, data type, and metadata link
project <- xpathSApply(nodes[[i]], "//td", xmlValue)[((i-1)*5)+1]
name <- xpathSApply(nodes[[i]], "//td", xmlValue)[((i-1)*5)+2]
type <- xpathSApply(nodes[[i]], "//td", xmlValue) [((i-1)*5)+3]
dataLink <- paste("http://gdr.agg.nrcan.gc.ca/gdrdap/dap/search-eng.php?tree-0=Click+here+for+more+options&tree-1=Click+here+for+more+options&tree-2=Click+here+for+more+options&tree-3=Click+here+for+more+options&datatype-ddl=&layer_name=", URLencode(name, reserved = TRUE), "&submit_search=Submit+Search#results", sep="")
metaLink <- xpathSApply(nodes[[i]], "//td/a[@href]", xmlGetAttr, "href")[i]
metaLink <- paste("http://gdr.agg.nrcan.gc.ca/gdrdap/dap/", metaLink, sep="")
## build temporary data frame, and merge with existing data frame
temp <- data.frame(project, name, type, dataLink, metaLink, stringsAsFactors = FALSE)
if (is.null(data)) data <- temp
else data <- rbind(data, temp)
}
data
}
NRCANGeophysicsScrape()
?table
setwd("C:/Papers/Coursera/Reproducible Research/RepData_PeerAssessment1")
?read.csv
?summary
